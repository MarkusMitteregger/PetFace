{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be680658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from model.EfficientNet import EfficientNet\n",
    "from trainer.process_data import CatTripletDataset, load_cat_data\n",
    "from trainer.evaluate import evaluate_model  # NEW: Import evaluation module\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0f5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "TRAINING CONFIGURATION\n",
      "------------------------------------------------------------\n",
      "BATCH_SIZE                16\n",
      "EMBEDDING_DIM             128\n",
      "NUM_EPOCHS                50\n",
      "LEARNING_RATE             0.0001\n",
      "SUBSET_SIZE               100\n",
      "TRIPLETS_PER_EPOCH        1000\n",
      "VAL_SUBSET_SIZE           50\n",
      "VAL_TRIPLETS              500\n",
      "MARGIN                    1.0\n",
      "PATIENCE                  20\n",
      "DATASET_PATH              ./cat\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Hyperparameters\n",
    "CONFIG = {\n",
    "    'BATCH_SIZE': 16,\n",
    "    'EMBEDDING_DIM': 128,\n",
    "    'NUM_EPOCHS': 50,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SUBSET_SIZE': 100,  # Number of cats to use per epoch\n",
    "    'TRIPLETS_PER_EPOCH': 1000,  # Number of triplets per epoch\n",
    "    'VAL_SUBSET_SIZE': 50,\n",
    "    'VAL_TRIPLETS': 500,\n",
    "    'MARGIN': 1.0,  # Triplet loss margin\n",
    "    'PATIENCE': 20,  # Early stopping patience\n",
    "    'DATASET_PATH': './cat',\n",
    "}\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"-\"*60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key:<25} {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86bce76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "Loading data from: ./cat\n",
      "Found 164100 cat folders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cats: 100%|██████████| 164100/164100 [00:14<00:00, 10969.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 643539 images from 164100 unique cats\n",
      "\n",
      "Total images: 643,539\n",
      "Total unique cats: 164,100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading dataset...\")\n",
    "image_paths, labels = load_cat_data(CONFIG['DATASET_PATH'])\n",
    "\n",
    "print(f\"\\nTotal images: {len(image_paths):,}\")\n",
    "print(f\"Total unique cats: {len(set(labels)):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5f3650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique cats: 164,100\n",
      "Train cats: 131,280\n",
      "Val cats: 32,820\n",
      "\n",
      "Training images: 515,159\n",
      "Validation images: 128,380\n",
      "Training cats: 131,280\n",
      "Validation cats: 32,820\n"
     ]
    }
   ],
   "source": [
    "unique_cats = sorted(list(set(labels)))\n",
    "print(f'Total unique cats: {len(unique_cats):,}')\n",
    "\n",
    "# Split cats 80/20\n",
    "train_cat_ids, val_cat_ids = train_test_split(\n",
    "    unique_cats, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_cat_ids = set(train_cat_ids)\n",
    "val_cat_ids = set(val_cat_ids)\n",
    "\n",
    "print(f'Train cats: {len(train_cat_ids):,}')\n",
    "print(f'Val cats: {len(val_cat_ids):,}')\n",
    "\n",
    "# Split images based on cat ID\n",
    "train_paths, train_labels = [], []\n",
    "val_paths, val_labels = [], []\n",
    "\n",
    "for img_path, cat_id in zip(image_paths, labels):\n",
    "    if cat_id in train_cat_ids:\n",
    "        train_paths.append(img_path)\n",
    "        train_labels.append(cat_id)\n",
    "    else:\n",
    "        val_paths.append(img_path)\n",
    "        val_labels.append(cat_id)\n",
    "\n",
    "print(f'\\nTraining images: {len(train_paths):,}')\n",
    "print(f'Validation images: {len(val_paths):,}')\n",
    "print(f'Training cats: {len(set(train_labels)):,}')\n",
    "print(f'Validation cats: {len(set(val_labels)):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79560794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the SIZE as needed\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20574453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cats: 131280\n",
      "Total images: 515159\n",
      "Sampling from 100 cats...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating triplets: 100%|██████████| 1000/1000 [00:00<00:00, 136284.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triplets generated: 1000\n",
      "Total cats: 32820\n",
      "Total images: 128380\n",
      "Sampling from 50 cats...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating triplets: 100%|██████████| 500/500 [00:00<00:00, 173075.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triplets generated: 500\n",
      "\n",
      "✓ Train batches: 63\n",
      "✓ Val batches: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CatTripletDataset(\n",
    "    train_paths, \n",
    "    train_labels, \n",
    "    transform=train_transform,\n",
    "    subset_size=CONFIG['SUBSET_SIZE'],\n",
    "    triplets_per_epoch=CONFIG['TRIPLETS_PER_EPOCH']\n",
    ")\n",
    "\n",
    "val_dataset = CatTripletDataset(\n",
    "    val_paths, \n",
    "    val_labels, \n",
    "    transform=val_transform,\n",
    "    subset_size=CONFIG['VAL_SUBSET_SIZE'],\n",
    "    triplets_per_epoch=CONFIG['VAL_TRIPLETS']\n",
    ")\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['BATCH_SIZE'], \n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG['BATCH_SIZE'], \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'\\n✓ Train batches: {len(train_loader):,}')\n",
    "print(f'✓ Val batches: {len(val_loader):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6b7c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viti1igo/anaconda3/envs/PetFace/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/viti1igo/anaconda3/envs/PetFace/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SUMMARY\n",
      "------------------------------------------------------------\n",
      "Total parameters:     4,730,364\n",
      "Trainable parameters: 4,730,364\n",
      "Model size:           18.04 MB\n",
      "Device:               cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet(embedding_dim=CONFIG['EMBEDDING_DIM'])\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(f'{\"-\"*60}')\n",
    "print(f'Total parameters:     {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "print(f'Model size:           {total_params * 4 / 1024 / 1024:.2f} MB')\n",
    "print(f'Device:               {next(model.parameters()).device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d1e6e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loss: Triplet Margin Loss (margin=1.0)\n",
      "✓ Optimizer: Adam (lr=0.0001)\n",
      "✓ Scheduler: StepLR (step=10, gamma=0.5)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.TripletMarginLoss(margin=CONFIG['MARGIN'], p=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(f\"\\n✓ Loss: Triplet Margin Loss (margin={CONFIG['MARGIN']})\")\n",
    "print(f\"✓ Optimizer: Adam (lr={CONFIG['LEARNING_RATE']})\")\n",
    "print(f\"✓ Scheduler: StepLR (step=10, gamma=0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c8b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for anchor, positive, negative in pbar:\n",
    "        # Move to device\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_emb = model(anchor)\n",
    "        pos_emb = model(positive)\n",
    "        neg_emb = model(negative)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(anchor_emb, pos_emb, neg_emb)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Validating\")\n",
    "    for anchor, positive, negative in pbar:\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        \n",
    "        anchor_emb = model(anchor)\n",
    "        pos_emb = model(positive)\n",
    "        neg_emb = model(negative)\n",
    "        \n",
    "        loss = criterion(anchor_emb, pos_emb, neg_emb)\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b24581b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Epoch 1/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/63 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 1.94 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 97.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m anchor, positive, negative \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Move to device\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     anchor \u001b[38;5;241m=\u001b[39m \u001b[43manchor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     positive \u001b[38;5;241m=\u001b[39m positive\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     negative \u001b[38;5;241m=\u001b[39m negative\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 1.94 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 97.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(CONFIG['NUM_EPOCHS']):\n",
    "    print(f'\\nEpoch {epoch+1}/{CONFIG[\"NUM_EPOCHS\"]}')\n",
    "    print('-' * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    print(f'\\nTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6f}')\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG\n",
    "        }, 'best_efficientnet_triplet.pth')\n",
    "        print(f'✓ New best val loss: {best_val_loss:.4f} - Model saved!')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'No improvement for {patience_counter} epochs')\n",
    "    \n",
    "    if patience_counter >= CONFIG['PATIENCE']:\n",
    "        print(f'\\n⚠ Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "\n",
    "print('\\n' + \"=\"*60)\n",
    "print('✓ TRAINING COMPLETED!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PetFace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
